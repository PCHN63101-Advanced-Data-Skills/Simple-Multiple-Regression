{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4a964b-a2bc-4d77-b1e8-6928da6b2b73",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this lesson, we have covered a lot of information around regression models. Starting with the idea of drawing a line through a cloud of points, we then formalised this approach using the statistical modelling framework discussed last week. This led us to the *simple linear regression* model, characterised by assuming a normal distribution with a mean function defined by a straight line and a constant variance function. We then discussed how the population parameters that define this line can be *estimated* from a sample of data. Although typically taught through the lens of least-squares, we instead introduced *maximum likelihood* as a more general-purpose approach to estimation that is grounded in probability. After discussing the interpretation of these estimates, we then turned to expanding *simple* regression into *multiple* regression. This transforms the regression *line* into a regression *plane*. However, most of the information from simple regression remains applicable. The only major difference concerns the *interpretation* and subsequent *plotting* of the model parameters.\n",
    "\n",
    "Next week, we will stick to the topic of multiple regression and discuss *statistical inference* using the parameter estimates. This is one of the more controversial aspects of classical statistics, as we will need to discuss $p$-values both in terms of their interpretation, but also in terms of their many flaws. This is central to the debate on the appropriate use of statistics in Psychology and so is a topic of great importance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}