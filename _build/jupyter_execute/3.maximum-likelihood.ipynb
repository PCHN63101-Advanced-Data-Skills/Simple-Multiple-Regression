{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "As a brief review, we are now in the position where we have specified the form of model we wish to use on our data. For some continuous outcome variable and some continuous predictor, we start with simplest relationship we can imagine: a straight-line. We then formalise this by placing it within the context of a statistical model, giving:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    y_{i}   &= \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\\\\n",
    "    \\epsilon_{i} &\\sim \\mathcal{N}\\left(0,\\sigma^{2}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is all fine, except that we have not actually done anything yet! All we have done is written down the model equation that we would like to use. However, we cannot do anything with this because it contains *unknown values*. At the point of analysing the data, we have measurements of both $y$ and $x$, but we do *not* have values for:\n",
    "\n",
    "- The intercept $\\beta_{0}$\n",
    "- The slope $\\beta_{1}$\n",
    "- The errors $\\epsilon_{i}$ &ndash; because they depend upon $\\beta_{0}$ and $\\beta_{1}$\n",
    "- The variance $\\sigma^{2}$ &ndash; because it depends upon the errors\n",
    "\n",
    "So, we are currently a bit stuck. Earlier in this lesson, we saw a way of *estimating* the slope and intercept using the method of *least-squares*. In this section, we will examine a more generic way of arriving at these values using the *Method of Maximum Likelihood*. In effect, this will provide us with values for all the unknowns above, allowing us to actually perform calculations and reach conclusions using our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7120c",
   "metadata": {},
   "source": [
    "## Notation for Estimates\n",
    "Before getting to the details of estimation, it is important that we establish some new notation for the estimates themseleves. So far, our notation has implicitly assumed that we are referring to the whole popluation under study. As such, the parameters indicated above represent *population-level constants*. In other words, $\\beta_{0}$ is the *true* intercept and $\\beta_{1}$ is the *true* slope. In reality, however, we will usually only have a *sample* from a population. As such, the parameters we calculate will only be *estimates* of the true values.\n",
    "\n",
    "Traditionally, we denote a parameter estimate by placing a \"hat\" on top of the corresponding Greek letter. For instance, we would denote an estimate of $\\beta_{1}$ from a given sample as $\\hat{\\beta}_{1}$ (pronounced \"beta 1 hat\"). This is important because whenever we see $\\hat{\\beta}_{1}$, this tells us that the value is effectively a *guess* based on a certain sample of data. In comaprison, whenever we see $\\beta_{1}$, this indicates the *true* population value, even though this is largely theoretical and unknowable quantity. We will see more later about why this distinction is important. For the moment, you just need to keep an eye on those little hats because they make quite a big difference to the meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bbdeee",
   "metadata": {},
   "source": [
    "`````{admonition} Alternative Notation for Estimates\n",
    ":class: tip\n",
    "Some authors like to use the Latin alternatives to the Greek letters to denote estimates. We will not be doing this, but it is worth knowing about to make sense of notation you may see in textbooks or the literature. In this scheme, the estimate of $\\beta_{1}$ would be $b_{1}$. So, the true population model would be\n",
    "\n",
    "$$\n",
    "y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i},\n",
    "$$\n",
    "\n",
    "whereas as an estimated model based on data would be\n",
    "\n",
    "$$\n",
    "y_{i} = b_{0} + b_{1}x_{i} + e_{i}.\n",
    "$$\n",
    "\n",
    "You can decide which of these you prefer, but we will be wearing \"hats\" in all our notation going forward.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d9977",
   "metadata": {},
   "source": [
    "### Errors vs Residuals\n",
    "Another subtle but important difference is that, much like the parameters $\\beta_{0}$ and $\\beta_{1}$, the *errors* in the model above are defined at the level of the population. This is important, because their value depends upon $\\beta_{0}$ and $\\beta_{1}$. In other words, to get the *true* errors, we would need to know the *true* population values of the parameters. Because this is almost never possible, the errors we actually use are based on the *estimates* $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$. As such, the errors we calculate from our estimated model are going to be different from the true errors. We therefore make a distinction between *errors* and *residuals*. The *errors* are the differences from the *true* regression line, whereas the *residuals* are the differences from the *estimated* regression line. To denote this, residuals are often written with a Latin $e_{i}$, meaning we can write our *estimated* model as:\n",
    "\n",
    "$$\n",
    "y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} + e_{i}.\n",
    "$$\n",
    "\n",
    "This is helpful, because it makes it clearer that the residuals are not really a parameter, rather thay are a *derived* quantity. So we keep Greek letters wearing hats for our estimated parameters, and lower-case Latin letters for everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27a7e3",
   "metadata": {},
   "source": [
    "## Calculating Estimates with Maximum Likelihood\n",
    "Now that we have established some new notation and terminology, we can turn to the main topic of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c901a",
   "metadata": {},
   "source": [
    "### Why Maximum Likelihood?\n",
    "\n",
    "The main point of ML estimation is that it is *generic*. OLS can only be applied in cases of linear regression, whereas ML is used for Linear Models, Generalised Linear Model, Mixed-effects Models and Generalised Mixed-effects Models. The principles of the *likelihood* are also very important for understanding Bayesian Statistics. So, it is more helpful in the long-run to understand ML, even if OLS is easier to understand. We will see ML cropping-up throughout this course, so we may as well start by trying to understand it, rather than having to back-track later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9725c",
   "metadata": {},
   "source": [
    "### How Does Maximum Likelihood Work?\n",
    "...\n",
    "\n",
    "$$\n",
    "P\\left(\\mathcal{D}|\\boldsymbol{\\theta}\\right),\n",
    "$$\n",
    "\n",
    "where $\\mathcal{D}$ represents the data we have collected and $\\boldsymbol{\\theta}$ is a generic representation of any set of parameters. For instance, in simple regression, $\\boldsymbol{\\theta} = \\{\\beta_{0},\\beta_{1},\\sigma^{2}\\}$. So the quantity of interest is the probability of the data given some values of the parameters[^bayesfoot].\n",
    "\n",
    "The key point to understand about ML is that it is based on evaluating *the probability of the data*, given some values of the parameters. So we can think about it as taking a guess for the parameter values, then calculating how probable those values make the data we have collected. It is like asking the question: \"how likely would it have been to collect the data that we have collected, if these were the parameter values?\". By searching through lots of different possible combinations of parameter values, the aim is to find the specific combination that leads to the *highest probability* of the data. Formally, we can write\n",
    "\n",
    "$$\n",
    "\\left(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\hat{\\sigma}^{2}\\right) = \\text{arg max}\\hspace{0.5em}  \\mathcal{l}\\left(\\beta_{0}, \\beta_{1}, \\sigma^{2}\\right),\n",
    "$$\n",
    "\n",
    "which is just saying that our parameter estimates are those values that make the output of the likelihood function $\\mathcal{l}\\left(\\beta_{0}, \\beta_{1}, \\sigma^{2}\\right)$ as *big* as possible.\n",
    "\n",
    "As an example, let us use the `mtcars` data again. Furthermore, let us say that we have guessed that $\\beta_{0} = 30$, $\\beta_{1} = -5$ and $\\sigma^{2} = 1$. Do not worry too much about where these guesses have come from, they are just an example. If we are assuming that the data have come from a normal distribution, we can therefore calculate the probability of the first value of `mpg` using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71308143",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 8.926166e-05\n"
     ]
    }
   ],
   "source": [
    "beta.0 <- 30\n",
    "beta.1 <- -5\n",
    "mu     <- beta.0 + beta.1*mtcars$wt[1]\n",
    "sigma2 <- 1\n",
    "lik    <- dnorm(mtcars$mpg[1], mean=mu, sd=sqrt(sigma2))\n",
    "\n",
    "print(lik)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee629e1",
   "metadata": {},
   "source": [
    "where the function `dnorm` returns the *density* of the normal distribution for the given data. This is the area under the normal curve, equivalent to the *probability* of a specific value. The probability of the second value of `mpg` would then be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3489eab3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 2.125155e-07\n"
     ]
    }
   ],
   "source": [
    "mu  <- beta.0 + beta.1*mtcars$wt[2]\n",
    "lik <- dnorm(mtcars$mpg[2], mean=mu, sd=sqrt(sigma2))\n",
    "\n",
    "print(lik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b9d55",
   "metadata": {},
   "source": [
    "and so on. The find out the overall likelihood for the *whole* dataset, we just need to *multiply* these probabilities. However, this can cause computational problems[^likprobsfoot], so it is more usual to sum the *log* of these probabilities to give the *log likelihood*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf0f642",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] -780.7884\n"
     ]
    }
   ],
   "source": [
    "mu     <- beta.0 + beta.1*mtcars$wt\n",
    "loglik <- 0\n",
    "loglik <- sum(dnorm(mtcars$mpg, mean=mu, sd=sqrt(sigma2), log=TRUE))\n",
    "print(loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959a796",
   "metadata": {},
   "source": [
    "This value is not particularly interpretable, but this does not matter. All we want to do is make it as *big* as possible. Because this has returned a *negative* value, what we need to do is make is as *positive* as possible. So maybe we should try something else? \n",
    "\n",
    "Let us inch closer to the results we got from least-squares earlier by setting $\\beta_{0} = 35$, $\\beta_{1} = -5$ and $\\sigma^{2} = 1$. This gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e460af",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] -192.4884\n"
     ]
    }
   ],
   "source": [
    "beta.0 <- 35\n",
    "beta.1 <- -5\n",
    "mu     <- beta.0 + beta.1*mtcars$wt\n",
    "sigma2 <- 1\n",
    "loglik <- 0\n",
    "\n",
    "loglik <- sum(dnorm(mtcars$mpg, mean=mu, sd=sqrt(sigma2), log=TRUE))\n",
    "print(loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9b0be",
   "metadata": {},
   "source": [
    "So, the log likelihood is now *more positive*, meaning we are moving in the right direction. So hopefully it is clear how calculating the log-likelihood gives us a metric of which combination of parameters make our data the *most probable*. Now, importantly, this depends upon the assumed distribution of the data, so we can start to see how the assumptions we made when defining our model are starting to be used. We can also see that this depends upon the data we have collected. So, the assumption here is that our sample is *representative* in order to make sure the estimates are close to the population values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c47525",
   "metadata": {},
   "source": [
    "### Optimisation Algorithms\n",
    "Of course, searching through many combinations of guesses for the parameters is not particularly efficient or principled. In order to do this sensibly, we rely on computer algorithms to search through many possible combinations of values to find which one *maximises* the log-likelihood. These are known as *optimisation* algorithms and are a complex topic in numerical computing. For us, we do not really need to understand how these work. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f48b2fd5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   beta.0    beta.1 \n",
      "37.285126 -5.344472 \n"
     ]
    }
   ],
   "source": [
    "set.seed(123)\n",
    "x <- mtcars$wt\n",
    "y <- mtcars$mpg\n",
    "\n",
    "# Define negative log-likelihood\n",
    "neg_loglik <- function(params) {\n",
    "  beta.0 <- params[1]\n",
    "  beta.1 <- params[2]\n",
    "  sigma  <- params[3]\n",
    "  mu     <- beta.0 + beta.1*x\n",
    "\n",
    "  if (sigma <= 0) return(Inf)  # log-likelihood undefined for sigma â‰¤ 0\n",
    "  \n",
    "  -sum(dnorm(y, mean=mu, sd=sigma, log=TRUE))\n",
    "}\n",
    "\n",
    "# Starting values (a guess based on plotting the data)\n",
    "init_params <- c(37,-5,3)\n",
    "\n",
    "# Run optimisation\n",
    "mle <- optim(\n",
    "  par     = init_params,\n",
    "  fn      = neg_loglik,\n",
    "  method  = \"BFGS\",\n",
    "  control = list(reltol = 1e-12),\n",
    "  hessian = TRUE\n",
    ")\n",
    "\n",
    "# Print results\n",
    "mle_est <- mle$par[1:2]\n",
    "names(mle_est) <- c(\"beta.0\", \"beta.1\")\n",
    "print(mle_est)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134b625",
   "metadata": {},
   "source": [
    "Which we can compare to the results `R` gives us when using the `lm()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20952da7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Intercept)          wt \n",
      "  37.285126   -5.344472 \n"
     ]
    }
   ],
   "source": [
    "print(coef(lm(mpg ~ wt, data=mtcars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b9251",
   "metadata": {},
   "source": [
    "### Closed-form Solutions\n",
    "For certain applications of ML, the need to use an iterative algorithm is unnecessary. This is because evaluating the values maximise the likelihood function can be worked-out and an equation giving the solution specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472706f",
   "metadata": {},
   "source": [
    "### Restricted Maximum Likelihood\n",
    "In the example above, your may have notice that we neglected to show the estimates for $\\sigma^{2}$. This was not an accident...\n",
    " \n",
    "Sometimes denoted REML or ReML, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b25a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`````{admonition} Degrees of Freedom\n",
    ":class: tip\n",
    "...\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe427fb",
   "metadata": {},
   "source": [
    "## Maximum Likelihood vs Least-Squares\n",
    "\n",
    "It is important to understand that the implementation of basic linear models in software almost always use OLS instead of ML or REML. So for us, it is important to remember that `R` does *not* use ML or REML estimation for the `lm()` function, even though it *could*. In this context, OLS is easier and simpler to estimate, and is more computationally efficient because it does not require iteration. However, because the results are *the same*, there is no harm viewing linear models through the lens of ML, because this allows for a very general perspective that will be useful as models get more complex in future[^searlefoot]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a616dc5",
   "metadata": {},
   "source": [
    "## The Final Estimated Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc56e5e",
   "metadata": {},
   "source": [
    "[^searlefoot]: This is the perspective taken by [McCulloch, Searle & Neuhaus (2008)](https://www.librarysearch.manchester.ac.uk/permalink/44MAN_INST/1r887gn/alma9930787964401631), who are leading experts on the use of linear models and their derivatives within statistics. This book gives everything you need to understand about the mathematical theory behind this framework, though it is not for the faint of heart!\n",
    "\n",
    "[^likprobsfoot]: This is often a problem of just getting values of 0 due to issues with computational precision when working with many small probabilities. Taking logs not only changes the scale so that this does not happen, but it also turns *multiplication* into *summation*. Historically, this made calculating the likelihood much easier by hand. If you ever want to get back to the likelihood value, you can just undo the logs by using `exp(loglik)`.\n",
    "\n",
    "[^bayesfoot]: This may seem like the wrong quantity. Surely, we are interested in $P(\\boldsymbol{\\theta}|\\mathcal{D})$? In other words, finding the parameters that are most probable, given the data. Unfortunately, evaluating the probability $P(\\boldsymbol{\\theta}|\\mathcal{D})$ requires Bayesian methods. Because Fisher hated Bayesian statistics, he was determined to find methods of estimation that did not require Bayes Theorem. Hence, the likelihood was adopted as a method that could be used from a purely Frequentist perspective. We will see more about this later on the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e64b7a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}