{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "As a brief review, we are now in the position where we have specified the form of model we wish to use on our data. For some continuous outcome variable and some continuous predictor, we start with most simple relationship we can image: a straight-line. We then formalise this by placing it within the context of a statistical model, giving:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    y_{i}   &= \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\\\\n",
    "    \\epsilon_{i} &\\sim \\mathcal{N}\\left(0,\\sigma^{2}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is all fine, except that we have not actually done anything yet! All we have done is written down the model equation that we would like to use. However, we cannot do anything with this because it contains *unknown values*. At the point of analysing the data, we have measurements of both $y$ and $x$, but we do *not* have values for:\n",
    "\n",
    "- The intercept ($\\beta_{0}$)\n",
    "- The slope ($\\beta_{1}$)\n",
    "- The errors (because these depend upon $\\beta_{0}$ and $\\beta_{1}$)\n",
    "- The variance (because this depends upon the errors)\n",
    "\n",
    "So, we are currently a bit stuck. Earlier in this lesson, we saw a way of *estimating* the slope and intercept using the method of *least-squares*. In this section, we will examine a more generic way of arriving at these values using the *Method of Maximum Likelihood*. In effect, this will provide us with values for all the unknowns above, allowing us to actually perform calculations and reach conclusions using our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7120c",
   "metadata": {},
   "source": [
    "## Notation for Estimates\n",
    "Before getting to the details of estimation, it is important that we establish some new notation for the estimates themseleves. So far, our notation has implicitly assumed that we are referring to the whole popluation under study. As such, the parameters indicated above represent *population-level constants*. In other words, $\\beta_{0}$ is the *true* intercept and $\\beta_{1}$ is the *true* slope. In reality, however, we will usually only have a *sample* from a population. As such, the parameters we calculate will only be *estimates* of the true values.\n",
    "\n",
    "Traditionally, we denote a parameter estimate by placing a \"hat\" on top of the corresponding Greek letter. For instance, we would denote an estimate of $\\beta_{1}$ from a given sample as $\\hat{\\beta}_{1}$ (pronounced \"beta 1 hat\"). This is important because whenever we see $\\hat{\\beta}_{1}$, this tells us that the value is effectively a *guess* based on a certain sample of data. In comaprison, whenever we see $\\beta_{1}$, this indicates the *true* population value, even though this is largely theoretical and unknowable quantity. \n",
    "\n",
    "The importance of this distinction then goes further than just indicating that $\\hat{\\beta}_{1}$ is an estimate. Because $\\hat{\\beta}_{1}$ will change for every new sample we take, its value will shift around. As such, $\\hat{\\beta}_{1}$ is a *random variable*. This is important, because $\\beta_{1}$ is *constant* and $\\hat{\\beta}_{1}$ is *random*. As such, $\\hat{\\beta}_{1}$ has a distribution with an expectation and a variance. We will get into the specifics of this later, but for the moment it is worth noting that an expression such as $E\\left(\\hat{\\beta}_{1}\\right)$ and $\\text{Var}\\left(\\hat{\\beta}_{1}\\right)$ make complete sense because our estimates are *random*, whereas $E\\left(\\beta_{1}\\right)$ and $\\text{Var}\\left(\\beta_{1}\\right)$ do not make sense because the true population values are *constant*. The main take-away here is that you need to keep an eye on those little hats because they make quite a big difference to the meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bbdeee",
   "metadata": {},
   "source": [
    "`````{admonition} Alternative Notation for Estimates\n",
    ":class: tip\n",
    "Some authors like to use the Latin alternatives to the Greek letters to denote estimates. We will not be doing this, but it is worth knowing about to make sense of notation you may see in textbooks or the literature. In this scheme, the estimate of $\\beta_{1}$ would be $b_{1}$. So, the true population model would be\n",
    "\n",
    "$$\n",
    "y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i},\n",
    "$$\n",
    "\n",
    "whereas as an estimated model based on data would be\n",
    "\n",
    "$$\n",
    "y_{i} = b_{0} + b_{1}x_{i} + e_{i}.\n",
    "$$\n",
    "\n",
    "You can decide which of these you prefer, but we will be wearing \"hats\" in all our notation going forward.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d9977",
   "metadata": {},
   "source": [
    "### Errors vs Residuals\n",
    "Another subtle but important difference that we need to understand is that, much like the parameters $\\beta_{0}$ and $\\beta_{1}$, the *errors* in the model above are defined at the level of the population. This is important, because their value depends upon $\\beta_{0}$ and $\\beta_{1}$. In other words, to get the *true* errors, we would need to know the *true* population values of the parameters. Because this is almost never possible, the errors we actually use are based on the *estimates* $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$. As such, the errors we calculate from our estimated model and likely to be different from the true errors. As such, we make a distinction between *errors* and *residuals*. The *errors* are the true but unknown differences from the population regression line, whereas the *residuals* are the estimated errors based on the estimated regression line.\n",
    "\n",
    "In terms of notation, residuals are often denoted with a Latin $e_{i}$, meaning we can write our *estimated* model as:\n",
    "\n",
    "$$\n",
    "y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} + e_{i}.\n",
    "$$\n",
    "\n",
    "This is helpful as well, because it makes it clearer that the residuals are not really a parameter, rather thay are a *derived* quantity. So we keep Greek letters wearing hats for our estimated parameters, and lower-case Latin letters for everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f6ee5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`````{admonition} Residuals are Not Independent with Constant Variance\n",
    ":class: tip\n",
    "Another subtle, but important, distinction between *errors* and *residuals* is that the estimation process actually changes the distributional properties of the residuals, compared to the errors. So while it is correct to assume\n",
    "\n",
    "$$\n",
    "\\epsilon_{i} \\sim \\mathcal{N}\\left(0,\\sigma^{2}\\right),\n",
    "$$\n",
    "\n",
    "it is *not* technically correct to assume the same for the *errors*. This is because the estimation procedure can *induce* correlation between the errors and the errors can have non-constant variance, depending upon a property known as *leverage*. We will discuss some of these concepts next week. For now, just note that the residuals can be used as an *approximation* for the errors, but we need to perform some additional checks to make sure that this approximation is reasonable.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27a7e3",
   "metadata": {},
   "source": [
    "## Calculating Estimates with Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c901a",
   "metadata": {},
   "source": [
    "### Why Maximum Likelihood?\n",
    "\n",
    "The main point of ML estimation is that it is *generic*. OLS can only be applied in cases of linear regression, whereas ML is used for Linear Models, Generalised Linear Model, Mixed-effects Models and Generalised Mixed-effects Models. The principles of the *likelihood* are also very important for understanding Bayesian Statistics. So, it is more helpful in the long-run to understand ML, even if OLS is easier to understand. We will see ML cropping-up throughout this course, so we may as well start by trying to understand it, rather than having to back-track later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9725c",
   "metadata": {},
   "source": [
    "### How Does Maximum Likelihood Work?\n",
    "The key point to understand about ML is that it is based on evaluating *the probability of the data*, given some values of the parameters. So we can think about it as taking a guess for the parameter values, then calculating how probable those values make the data we have collected. It is like asking the question: \"how likely would it have been to collect the data that we have collected, if these were the parameter values?\". By searching through lots of different possible combinations of parameter values, the aim is to find the specific combination that leads to the *highest probability* of the data. Formally, we can write\n",
    "\n",
    "$$\n",
    "\\left(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\hat{\\sigma}^{2}\\right) = \\text{arg max}\\hspace{0.5em}  \\mathcal{l}\\left(\\beta_{0}, \\beta_{1}, \\sigma^{2}\\right),\n",
    "$$\n",
    "\n",
    "which is just saying that our parameter estimates are those values that make the output of the likelihood function $\\mathcal{l}\\left(\\beta_{0}, \\beta_{1}, \\sigma^{2}\\right)$ as *big* as possible.\n",
    "\n",
    "As an example, let us use the `mtcars` data again. Furthermore, let us say that we have guessed that $\\beta_{0} = 30$, $\\beta_{1} = -5$ and $\\sigma^{2} = 1$. Do not worry too much about where these guesses have come from, they are just an example. If we are assuming that the data have come from a normal distribution, we can therefore calculate the probability of the first value of `mpg` using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71308143",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 8.926166e-05\n"
     ]
    }
   ],
   "source": [
    "beta.0 <- 30\n",
    "beta.1 <- -5\n",
    "sigma2 <- 1\n",
    "lik    <- dnorm(mtcars$mpg[1], mean=(beta.0 + beta.1*mtcars$wt[1]), sd=sqrt(sigma2))\n",
    "print(lik)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee629e1",
   "metadata": {},
   "source": [
    "where the function `dnorm` returns the *density* of the normal distribution for the given data. This is the area under the normal curve, equivalent to the *probability* of a specific value. The probability of the second value of `mpg` would then be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3489eab3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 2.125155e-07\n"
     ]
    }
   ],
   "source": [
    "lik <- dnorm(mtcars$mpg[2], mean=beta.0 + beta.1*mtcars$wt[2], sd=sqrt(sigma2))\n",
    "print(lik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b9d55",
   "metadata": {},
   "source": [
    "and so on. The find out the overall likelihood for the *whole* dataset, we just need to *multiply* these probabilities. However, this can cause computational problems[^likprobsfoot], so it is more usual to sum the *log* of these probabilities to give the *log likelihood*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cf0f642",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] -780.7884\n"
     ]
    }
   ],
   "source": [
    "loglik <- 0\n",
    "loglik <- sum(dnorm(mtcars$mpg, mean=(beta.0 + beta.1*mtcars$wt), sd=sqrt(sigma2), log=TRUE))\n",
    "print(loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959a796",
   "metadata": {},
   "source": [
    "This value is not particularly interpretable, but this does not matter. All we want to do is make it as *big* as possible. Because this has returned a *negative* value, what we need to do is make is as *positive* as possible. So maybe we should try something else? \n",
    "\n",
    "Let us inch closer to the results we got from least-squares earlier by setting $\\beta_{0} = 35$, $\\beta_{1} = -5$ and $\\sigma^{2} = 1$. This gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24e460af",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] -192.4884\n"
     ]
    }
   ],
   "source": [
    "beta.0 <- 35\n",
    "beta.1 <- -5\n",
    "sigma2 <- 1\n",
    "loglik <- 0\n",
    "\n",
    "loglik <- sum(dnorm(mtcars$mpg, mean=(beta.0 + beta.1*mtcars$wt), sd=sqrt(sigma2), log=TRUE))\n",
    "print(loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9b0be",
   "metadata": {},
   "source": [
    "So, we have got *more positive* here and are therefore moving in the right direction. So hopefully it is clear how calculating the log-likelihood gives us a metric of which combination of parameters make our data the *most probable*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c47525",
   "metadata": {},
   "source": [
    "### Optimisation Algorithms\n",
    "Of course, searching through many combinations of guesses for the parameters is not particularly efficient or principled. In order to do this sensibly, we rely on computer algorithms to search through many possible combinations of values to find which one *maximises* the log-likelihood. These are known as *optimisation* algorithms and are a complex topic in numerical computing. For us, we do not really need to understand how these work. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f48b2fd5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    beta0     beta1     sigma \n",
      "37.285126 -5.344472  2.949163 \n"
     ]
    }
   ],
   "source": [
    "set.seed(123)\n",
    "x <- mtcars$wt\n",
    "y <- mtcars$mpg\n",
    "\n",
    "# Define negative log-likelihood\n",
    "neg_loglik <- function(params) {\n",
    "  beta0 <- params[1]\n",
    "  beta1 <- params[2]\n",
    "  sigma <- params[3]\n",
    "\n",
    "  if (sigma <= 0) return(Inf)  # log-likelihood undefined for sigma â‰¤ 0\n",
    "  \n",
    "  -sum(dnorm(y, mean=(beta0 + beta1 * x), sd=sigma, log=TRUE))\n",
    "}\n",
    "\n",
    "# Starting values (a guess based on plotting the data)\n",
    "init_params <- c(37,-5,3)\n",
    "\n",
    "# Run optimisation\n",
    "mle <- optim(\n",
    "  par     = init_params,\n",
    "  fn      = neg_loglik,\n",
    "  method  = \"BFGS\",\n",
    "  control = list(reltol = 1e-12),\n",
    "  hessian = TRUE\n",
    ")\n",
    "\n",
    "# Print results\n",
    "mle_est <- mle$par\n",
    "names(mle_est) <- c(\"beta0\", \"beta1\", \"sigma\")\n",
    "print(mle_est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20952da7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Intercept)          wt \n",
      "  37.285126   -5.344472 \n"
     ]
    }
   ],
   "source": [
    "print(coef(lm(mpg ~ wt, data=mtcars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b9251",
   "metadata": {},
   "source": [
    "### Closed-form Solutions\n",
    "For certain applications of ML, the need to use an iterative algorithm is unnecessary. This is because evaluating the values maximise the likelihood function can be worked-out and an equation giving the solution specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472706f",
   "metadata": {},
   "source": [
    "### Restricted Maximum Likelihood\n",
    "Sometimes denoted REML or ReML, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b25a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`````{admonition} Degrees of Freedom\n",
    ":class: tip\n",
    "...\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe427fb",
   "metadata": {},
   "source": [
    "## Maximum Likelihood vs Least-Squares\n",
    "\n",
    "It is important to understand that the implementation of basic linear models in software almost always use OLS instead of ML or REML. So for us, it is important to remember that `R` does *not* use ML or REML estimation for the `lm()` function, even though it *could*. In this context, OLS is easier and simpler to estimate, and is more computationally efficient because it does not require iteration. However, because the results are *the same*, there is no harm viewing linear models through the lens of ML, because this allows for a very general perspective that will be useful as models get more complex in future[^searlefoot]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc56e5e",
   "metadata": {},
   "source": [
    "[^searlefoot]: This is the perspective taken by [McCulloch, Searle & Neuhaus (2008)](https://www.librarysearch.manchester.ac.uk/permalink/44MAN_INST/1r887gn/alma9930787964401631), who are leading experts on the use of linear models and their derivatives within statistics. This book gives everything you need to understand about the mathematical theory behind this framework, though it is not for the faint of heart!\n",
    "\n",
    "[^likprobsfoot]: This is often a problem of just getting values of 0 due to issues with computational precision when working with many small probabilities. Taking logs not only changes the scale so that this does not happen, but it also turns *multiplication* into *summation*. Historically, this made calculating the likelihood much easier by hand. If you ever want to get back to the likelihood value, you can just undo the logs by using `exp(loglik)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e64b7a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}