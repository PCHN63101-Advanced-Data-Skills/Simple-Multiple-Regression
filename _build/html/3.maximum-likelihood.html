
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Maximum Likelihood &#8212; LM I: Simple and Multiple Regression</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '3.maximum-likelihood';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The Estimated Model" href="4.estimated-model.html" />
    <link rel="prev" title="The Simple Regression Model" href="2.simple-regression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="0.intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="LM I: Simple and Multiple Regression - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="LM I: Simple and Multiple Regression - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="0.intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1.straight-line.html">Fitting a Straight Line to Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.simple-regression.html">The Simple Regression Model</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Maximum Likelihood</a></li>
<li class="toctree-l1"><a class="reference internal" href="4.estimated-model.html">The Estimated Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="5.multiple-regression.html">Multiple Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="summary.html">Summary</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/PCHN63101-Advanced-Data-Skills/Simple-Multiple-Regression" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/PCHN63101-Advanced-Data-Skills/Simple-Multiple-Regression/issues/new?title=Issue%20on%20page%20%2F3.maximum-likelihood.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/3.maximum-likelihood.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Maximum Likelihood</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-vs-ols">MLE vs OLS</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-mle-work">How Does MLE Work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood-function">The Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-concrete-example-in-r">A Concrete Example in <code class="docutils literal notranslate"><span class="pre">R</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-solutions">Exact Solutions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-mle">Iterative MLE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#restricted-maximum-likelihood">Restricted Maximum Likelihood</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="maximum-likelihood">
<h1>Maximum Likelihood<a class="headerlink" href="#maximum-likelihood" title="Link to this heading">#</a></h1>
<p>At the end of the previous section, we discussed the concept of estimating the parameters of the simple regression model. In this section, we will explore a method of estimation known as <em>maximum likelihood estimation</em> (MLE) that is applicable to both simple linear models, as well as many of the more complex models we will see later on the course.</p>
<section id="mle-vs-ols">
<h2>MLE vs OLS<a class="headerlink" href="#mle-vs-ols" title="Link to this heading">#</a></h2>
<p>At the beginning of this lesson, we demonstrated how the simple regression line could be fit by minimising the sum of squared errors. This is known as the method of <em>ordinary least squares</em> (OLS). OLS is useful because it is easy to conceptualise and results in a simple set of equations for finding estimates. Because of this, OLS is generally considered <em>the</em> method used for estimating linear models. However, models that do not assume a normal distribution or those that assume more complex correlational and variance structures cannot be estimated with OLS. As such, OLS is actually something of a niche approach that gets abandoned quickly as models get more complex. Because of this, it is much more helpful to consider a <em>likelihood</em> approach to estimation<a class="footnote-reference brackets" href="#searlefoot" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. The principle of the likelihood is widely applicable across may different types of model, as well as being fundamental to Bayesian methods. As such, we can consider MLE as our singular generic method of estimation that will be applicable across this entire course.</p>
</section>
<section id="how-does-mle-work">
<h2>How Does MLE Work?<a class="headerlink" href="#how-does-mle-work" title="Link to this heading">#</a></h2>
<p>Fundamentally, MLE is based on finding parameter estimates that make a value, known as the <em>likelihood</em>, as <em>big</em> as possible. The value of the likelihood is calculated using the <em>likelihood function</em>, denoted</p>
<div class="math notranslate nohighlight">
\[
L\left(\theta|y\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> is a generic representation of any set of parameters. For instance, in simple regression, <span class="math notranslate nohighlight">\(\theta = \{\beta_{0},\beta_{1},\sigma^{2}\}\)</span>. In order to understand this, we can make an equivalence between the likelihood and probability. When placed in probabilitstic terms, we have</p>
<div class="math notranslate nohighlight">
\[
L\left(\theta|y\right) = P\left(y|\theta\right).
\]</div>
<p>In words, this is saying that the likelihood of a set of paramater values, given some data, is the <em>same</em> as evaluating the probability of the data, assuming those parameter values are true. The key point here is that MLE is about the <em>probability of the data</em><a class="footnote-reference brackets" href="#bayesfoot" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. We can think about it as taking a guess for the parameter values, then calculating how probable those guesses make the data we have collected. It is like asking the question: “how likely would it have been to collect my data, if these were the parameter values?”. By searching through lots of different possible combinations of parameter values, the aim is to find the specific combination that leads to the <em>highest probability</em> of the data. In other words, the values that <em>maxmimise the likelihood</em>.</p>
<section id="the-likelihood-function">
<h3>The Likelihood Function<a class="headerlink" href="#the-likelihood-function" title="Link to this heading">#</a></h3>
<p>So how do we evaluate the likelihood function? In brief, if we want the likelihood of our entire dataset, we need to <em>multiply</em> the probability of each datapoint, using some guesses for the parameter values. We can then repeat this for some other guesses. If the likelihood gets <em>larger</em> then our new guesses make the data more probable than our old guesses.</p>
<p>So how do we calculate the probabilities of each data point? Remember that our core assumption for simple regression is that</p>
<div class="math notranslate nohighlight">
\[
y_{i} \sim \mathcal{N}\left(\beta_{0} + \beta_{1}x_{i}, \sigma^{2}\right).
\]</div>
<p>The probability of any value from a normal distribution can be calculated using its <em>probability density function</em>, which is given by</p>
<div class="math notranslate nohighlight">
\[
f(y) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(y - \mu)^{2}}{2\sigma^{2}}\right).
\]</div>
<p>This can look a little complicated, but notice that the only values we need to plug-in here are <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> and some value for <span class="math notranslate nohighlight">\(y\)</span>. For instance, if we wanted to know the probability of sampling a value of 10 from <span class="math notranslate nohighlight">\(\mathcal{N}(9,1)\)</span>, we can use</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="w">      </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">10</span>
<span class="n">mu</span><span class="w">     </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">9</span>
<span class="n">sigma2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span>

<span class="n">P.y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="o">/</span><span class="nf">sqrt</span><span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="kc">pi</span><span class="o">*</span><span class="n">sigma2</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="n">sigma2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">P.y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 0.2419707
</pre></div>
</div>
</div>
</div>
<p>By replacing <span class="math notranslate nohighlight">\(\mu\)</span> with the mean function, we can reframe the probability density using our model to give</p>
<div class="math notranslate nohighlight">
\[
P(y_{i}) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{\left(y_{i} - \left[\beta_{0} + \beta_{1}x_{i}\right]\right)^{2}}{2\sigma^{2}}\right).
\]</div>
<p>So, if we insert some guesses for <span class="math notranslate nohighlight">\(\beta_{0}\)</span>, <span class="math notranslate nohighlight">\(\beta_{1}\)</span> and <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>, we can calculate the probability of any of our data values. This effectively tells us the probability of that datapoint, assuming our guesses (and model) are correct.</p>
<p>Calculating the likelihood then involves multiplying all these values together to give</p>
<div class="math notranslate nohighlight">
\[
L(\beta_{0},\beta_{1},\sigma^{2}|\mathbf{y}) = P\left(y_{1}\right) \times P(y_{2}) \times \dots \times P(y_{n})
\]</div>
<p>which we can more compactly express using Big Pi notation (see the box below)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    L(\beta_{0},\beta_{1},\sigma^{2}|\mathbf{y}) &amp;= \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(y_{i} - [\beta_{0} + \beta_{1}x_{i}])^{2}}{2\sigma^{2}}\right) \\
    &amp;= \prod_{i=1}^{n} P(y_{i})
\end{align*}
\end{split}\]</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Big Pi Notation</p>
<p>Big pi notation, denoted by the captitol Greek letter <span class="math notranslate nohighlight">\(\Pi\)</span>, is used as a shorthand for <em>multiplication</em>. Below the big Pi, we define our indexing variable, as well as its starting value. Above the big Pi, we indicate the value where we stop. So the notation</p>
<div class="math notranslate nohighlight">
\[
P = \prod_{i=1}^{3} y_{i}
\]</div>
<p>is equivalent to</p>
<div class="math notranslate nohighlight">
\[
P = y_{1} \times y_{2} \times y_{3} ,
\]</div>
<p>So, the notation says that our index is called <span class="math notranslate nohighlight">\(i\)</span> and that we start it at <span class="math notranslate nohighlight">\(1\)</span>. We then keep going until <span class="math notranslate nohighlight">\(i = 3\)</span>, at which point we stop.</p>
<p>This has a direct connection to a for-loop. So, in code, this is the same as shortening</p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="m">3</span><span class="p">]</span>
</pre></div>
</div>
<p>to</p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">){</span>
<span class="w">    </span><span class="n">P</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">P</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>So, you can think of the Big Pi notation as a <em>multiplication loop</em> over a certain set of indices.</p>
</div>
</section>
<section id="a-concrete-example-in-r">
<h3>A Concrete Example in <code class="docutils literal notranslate"><span class="pre">R</span></code><a class="headerlink" href="#a-concrete-example-in-r" title="Link to this heading">#</a></h3>
<p>To get a more concrete sense of calculating the likelihood, let us use the <code class="docutils literal notranslate"><span class="pre">mtcars</span></code> data again. Furthermore, let us say that we have guessed that <span class="math notranslate nohighlight">\(\beta_{0} = 30\)</span>, <span class="math notranslate nohighlight">\(\beta_{1} = -5\)</span> and <span class="math notranslate nohighlight">\(\sigma^{2} = 1\)</span>. If we are assuming that the data have come from a normal distribution, we can calculate the probability of the first value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> using</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">beta.0</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">30</span>
<span class="n">beta.1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">-5</span>
<span class="n">sigma2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span>
<span class="n">mu</span><span class="w">     </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">beta.0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">beta.1</span><span class="o">*</span><span class="n">mtcars</span><span class="o">$</span><span class="n">wt</span><span class="p">[</span><span class="m">1</span><span class="p">]</span>
<span class="n">P.y1</span><span class="w">   </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dnorm</span><span class="p">(</span><span class="n">mtcars</span><span class="o">$</span><span class="n">mpg</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">P.y1</span><span class="w"> </span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 8.926166e-05
</pre></div>
</div>
</div>
</div>
<p>where the function <code class="docutils literal notranslate"><span class="pre">dnorm</span></code> returns the <em>density</em> of the normal distribution for the given data (so we do not need to calculate this manually, as we did earlier). The probability of the second value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> would then be</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">beta.0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">beta.1</span><span class="o">*</span><span class="n">mtcars</span><span class="o">$</span><span class="n">wt</span><span class="p">[</span><span class="m">2</span><span class="p">]</span>
<span class="n">P.y2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dnorm</span><span class="p">(</span><span class="n">mtcars</span><span class="o">$</span><span class="n">mpg</span><span class="p">[</span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">P.y2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 2.125155e-07
</pre></div>
</div>
</div>
</div>
<p>and so on. The find out the overall likelihood for the <em>whole</em> dataset, we just need to <em>multiply</em> these probabilities. However, this can cause computational problems<a class="footnote-reference brackets" href="#likprobsfoot" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>, so it is more usual to sum the <em>log</em> of these probabilities. This gives the <em>log-likelihood</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span><span class="w">     </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">beta.0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">beta.1</span><span class="o">*</span><span class="n">mtcars</span><span class="o">$</span><span class="n">wt</span>
<span class="n">loglik</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="nf">dnorm</span><span class="p">(</span><span class="n">mtcars</span><span class="o">$</span><span class="n">mpg</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="p">),</span><span class="w"> </span><span class="n">log</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">loglik</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] -780.7884
</pre></div>
</div>
</div>
</div>
<p>This value is not particularly interpretable, but this does not matter. All we want to do is make it as <em>big</em> as possible. In the context of a negative value, this means our aim is to make the likelihood as <em>positive</em> as possible. In principle, we just need to keep trying guesses for the parameters to see which ones make the log-likelihood as large as possible. Once we cannot make it any bigger, the estimation is complete.</p>
</section>
<section id="exact-solutions">
<h3>Exact Solutions<a class="headerlink" href="#exact-solutions" title="Link to this heading">#</a></h3>
<p>An obvious issue with the scheme above is that searching through many combinations of guesses for the parameters is not particularly efficient or practical. In fact, there is an infinite number of values we could choose, as well as an infinite number of combinations. So how can we possibly find the values we need? In order to do so, there are two options. For some simple problems, the equations that maximise the likelihood have already been worked out using the tools of calculus. As such, the equation for the likelihood can be solved to find those values that guarantee a maximum. Normal linear models are one such example. By assuming a Gaussian distribution for the outcome variables, the ML estimates for a simple regression model are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \hat{\beta}_{1} &amp;= \frac{\sum{\left(x_{i} - \bar{x}\right)\left(y_{i} - \bar{y}\right)}}{\sum{\left(x_{i} - \bar{x}\right)^{2}}}\\
    \hat{\beta}_{0} &amp;= \bar{y} - \hat{\beta}_{1}\bar{x}, \\ 
\end{align*}
\end{split}\]</div>
<p>which agree with what we saw earlier for OLS. As such, both MLE and OLS agree on the values of the intercept and slope. Because of this, we can conceptualise estimation of linear models either in terms of least-squares <em>or</em> in terms of the likelihood as, practically, the outcome is the same. Unfortunately, for more complex models, there are no exact solutions for maximising the likelihood. In these cases, we have to turn to computational methods in the form of <em>iterative</em> MLE.</p>
<div class="tip admonition">
<p class="admonition-title">MLE in Software</p>
<p>It is worth highlighting at this point that software implementations of linear models will use the exact estimating equations given above. This means they can be conceptualised as <em>either</em> using OLS or MLE. Typically, these implementations will be characterised as using OLS in documentation and textbooks as this is simpler, whereas using a full likelihood perspective is generally considered <em>overkill</em> for basic linear models. Nevertheless, this perspective have the advantage of being <em>generic</em>, meaning we only have to learn a single estimating framework. The only place where it differs is in terms of estimating <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>, but we will discuss that further below. If we wish to make any comparisons between OLS and MLE, it is also helpful to characterise the results of the <code class="docutils literal notranslate"><span class="pre">R</span></code> function <code class="docutils literal notranslate"><span class="pre">lm()</span></code> as representing OLS.</p>
</div>
</section>
<section id="iterative-mle">
<h3>Iterative MLE<a class="headerlink" href="#iterative-mle" title="Link to this heading">#</a></h3>
<p>For situations where it is not possible to find an exact solution, we rely on computer algorithms to search through many possible combinations of values to find which one <em>maximises</em> the log-likelihood. These are known more generally as <em>optimisation</em> algorithms and are a complex topic in numerical computing. For our purpose, we do not really need to understand how these algorithms work. All we really need to know is that they use rules and heuristics to explore the space of all possible parameter values in order to find values that the algorithm thinks make the log-likelihood the largest.</p>
<p>Within <code class="docutils literal notranslate"><span class="pre">R</span></code> the generic functions <code class="docutils literal notranslate"><span class="pre">optim()</span></code>, <code class="docutils literal notranslate"><span class="pre">nlm()</span></code> and <code class="docutils literal notranslate"><span class="pre">nlminb()</span></code> can all be used to do this. In the example below, we choose <code class="docutils literal notranslate"><span class="pre">nlm()</span></code> (<em>nonlinear minimisation</em>) for its general robustness for MLE problems. This function needs some starting guesses for the parameters, so in the example below we set <span class="math notranslate nohighlight">\(\hat{\beta}_{0} = \bar{y}\)</span>, <span class="math notranslate nohighlight">\(\hat{\beta}_{1} = 0\)</span> and <span class="math notranslate nohighlight">\(\sigma = \text{SD}(y)\)</span>. As the name implies, this function <em>minimises</em>, so we return the <em>negative</em> log-likelihood instead<a class="footnote-reference brackets" href="#minmaxfoot" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>. After running the iterative ML estimation, we get</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span>
<span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mtcars</span><span class="o">$</span><span class="n">wt</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mtcars</span><span class="o">$</span><span class="n">mpg</span>

<span class="c1"># Define negative log-likelihood</span>
<span class="n">neg_loglik</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">function</span><span class="p">(</span><span class="n">params</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">beta.0</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="m">1</span><span class="p">]</span>
<span class="w">  </span><span class="n">beta.1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="m">2</span><span class="p">]</span>
<span class="w">  </span><span class="n">sigma</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="m">3</span><span class="p">]</span>

<span class="w">  </span><span class="nf">if </span><span class="p">(</span><span class="n">sigma</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="nf">return</span><span class="p">(</span><span class="m">1e10</span><span class="p">)</span>

<span class="w">  </span><span class="n">mu</span><span class="w">     </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">beta.0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">beta.1</span><span class="o">*</span><span class="n">x</span><span class="w">    </span>
<span class="w">  </span><span class="n">loglik</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="nf">dnorm</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span><span class="w"> </span><span class="n">log</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">))</span><span class="w"> </span><span class="c1"># log-likelihood</span>
<span class="w">  </span><span class="nf">return</span><span class="p">(</span><span class="o">-</span><span class="n">loglik</span><span class="p">)</span><span class="w">                                      </span><span class="c1"># -ve log-likelihood</span>
<span class="p">}</span>

<span class="c1"># Starting values (guesses for intercept, slope and SD)</span>
<span class="n">init_params</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="nf">mean</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="nf">sd</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="c1"># Run optimisation</span>
<span class="n">mle</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">nlm</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">neg_loglik</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="o">=</span><span class="n">init_params</span><span class="p">)</span>

<span class="c1"># Print results</span>
<span class="n">mle_pars</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mle</span><span class="o">$</span><span class="n">estimate</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">]</span>
<span class="nf">names</span><span class="p">(</span><span class="n">mle_pars</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;beta.0&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;beta.1&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">mle_pars</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   beta.0    beta.1 
37.285127 -5.344472 
</pre></div>
</div>
</div>
</div>
<p>Which we can compare to the OLS results <code class="docutils literal notranslate"><span class="pre">R</span></code> gives us when using the <code class="docutils literal notranslate"><span class="pre">lm()</span></code> function</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="nf">coef</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">mpg</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">wt</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">mtcars</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(Intercept)          wt 
  37.285126   -5.344472 
</pre></div>
</div>
</div>
</div>
<p>As we can see, these are very close, showing how iterative MLE can be applied in many cases, even those where exact solutions exist. Although we do not need this for simple linear models, we will see later on the course how iterative MLE is necessary for Generalised Linear Models, Linear Mixed Models and Generalised Linear Mixed Models.</p>
</section>
<section id="restricted-maximum-likelihood">
<h3>Restricted Maximum Likelihood<a class="headerlink" href="#restricted-maximum-likelihood" title="Link to this heading">#</a></h3>
<p>In the examples above, your may have noticed that we neglected to show the estimates for <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>. This was not an accident. In terms of exact solutions, the variance estimates produced by OLS and MLE are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \hat{\sigma}^{2}_{\text{(OLS)}} &amp;= \frac{1}{n - k} \sum_{i=1}^{n} e_{i}^{2} \\
    \hat{\sigma}^{2}_{\text{(ML)}}  &amp;= \frac{1}{n} \sum_{i=1}^{n} e_{i}^{2} \\
\end{align*}
\end{split}\]</div>
<p>So, the OLS variance estimate implements a generalisation of Bessel’s correction, by dividing the sum of squared errors by <span class="math notranslate nohighlight">\(n - k\)</span> (where <span class="math notranslate nohighlight">\(k\)</span> is the number of parameters). This value is known as the <em>residual degrees of freedom</em>. If you are unfamiliar with concept of <em>degrees of freedom</em>, they are explroed in the drop-down box below. The ML estimate, by comparison, simply divides by <span class="math notranslate nohighlight">\(n\)</span>. Unless we have the entire population at our disposal, this will give a <em>biased</em> estimate for the variance, effectively <em>underestimating</em> it compared to the true value. We can see this bias in the results from iterative MLE, where the estimate was</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="n">mle</span><span class="o">$</span><span class="n">estimate</span><span class="p">[</span><span class="m">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 2.949163
</pre></div>
</div>
</div>
</div>
<p>compared to the OLS estimate of</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">mpg</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">wt</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">mtcars</span><span class="p">))</span><span class="o">$</span><span class="n">sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 3.045882
</pre></div>
</div>
</div>
</div>
<p>The reason this happens is because MLE estimates the variance without taking into account that <span class="math notranslate nohighlight">\(\hat{\beta}_{0}\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_{1}\)</span> are <em>estimates</em>. MLE treats these values as <em>known constants</em> and calculates the variance accordingly. As such, MLE is not <em>wrong</em> in its calculations, but it is wrong in its assumptions when estimating the variance. If these values were the true population values, then MLE would be correct. However, they are not.</p>
<p>In order to fix this, a variation of MLE was developed specifically for the unbiased estimation of variance, known as <em>restricted</em> maximum likelihood (REML). The way this works is a bit complicated and somewhat beyond the scope of this lesson. In effect, REML estimates the variance by first <em>removing</em> the effects of the predictors from the data. The resultant errors are then used to estimate the variance by running standard MLE (hence why it is sometimes known as <em>residual</em> maximum likelihood). The removal of the effects allows the correction to be automatically taken into account, meaning the variance estimate is <em>unbiased</em>. How this removal is achived gets quite complicated, but this should give you enough of a flavour of how REML works.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Degrees of Freedom</p>
<p>The value <span class="math notranslate nohighlight">\(n-k\)</span> in the denominator of the unbiased variance estimate is known as the <em>residual degrees of freedom</em>. If you have come across this concept before, you will known that it can be quite tricky to conceptualise. In effect, the degrees of freedom capture the number of independent pieces of information that are available for estimating some quantity. For example, consider a list of numbers of size <span class="math notranslate nohighlight">\(n\)</span>. If we then transform that list such that the sum of its values is 0, we are imposing a <em>constraint</em> on the number of possible values that list could contain. In this situation, <span class="math notranslate nohighlight">\(n-1\)</span> of the items in the list are free to have any value we want. However, in order for the constraint to be met, the final item in the list will be <em>forced</em> to have a particular value.</p>
<p>We can see this with an example. Take the following list of values</p>
<div class="math notranslate nohighlight">
\[
y = \left[1,2,6,8\right].
\]</div>
<p>Now imagine that we transform <span class="math notranslate nohighlight">\(y\)</span> to produce a new list, but impose the constraint that the sum of the values must be 0. This is done implicitly during mean-centering (i.e. subtracting the mean from each value), which would give</p>
<div class="math notranslate nohighlight">
\[
y^{\ast} = \left[-3.25, -2.25, 1.75, 3.75\right].
\]</div>
<p>Notice here that the sum of the first 3 values is <span class="math notranslate nohighlight">\(-3.25 + -2.25 + 1.75 = -3.75\)</span>, which is the same as the final value with the sign flipped. This is no coincidence. In fact, we can choose any arbitrary set of values for the first 3 numbers. For instance</p>
<div class="math notranslate nohighlight">
\[
y^{\ast} = \left[27, 4, -2, ?\right].
\]</div>
<p>The only limitation, is that the final value must negate the sum of these first 3 values, such that the overall sum is 0. This would give</p>
<div class="math notranslate nohighlight">
\[
y^{\ast} = \left[27, 4, -2, -29\right].
\]</div>
<p>So in this example, <span class="math notranslate nohighlight">\(y\)</span> has 4 degrees of freedom, but <span class="math notranslate nohighlight">\(y^{\ast}\)</span> has <span class="math notranslate nohighlight">\(n-1 = 3\)</span> degrees of freedom, because the value of element 4 is <em>constrained</em> by the fact it must sum to 0. This means that <span class="math notranslate nohighlight">\(y^{\ast}\)</span> cannot consist of any numbers, rather it is constrained to be one of the set of numbers where the final value negates the sum of the first 3. The final value is therefore <em>dependent</em> on the first 3 and there are only 3 independent pieces of information here, not 4.</p>
<p>The importance of this for statistical models is that the act of estimation <em>imposes a constraint</em> on the values that are produced. Although our outcomes <span class="math notranslate nohighlight">\(y\)</span> starts with <span class="math notranslate nohighlight">\(n\)</span> degrees of freedom, estimating parameters limits the number of possible values that can be produced by the model. For the residuals, their values are constrained by the parameter estimates. Each new parameter “uses up” a single degree of freedom. So if we have <span class="math notranslate nohighlight">\(k\)</span> parameters, then the number of residual degrees of freedom is <span class="math notranslate nohighlight">\(n-k\)</span>. For simple regression, <span class="math notranslate nohighlight">\(k = 2\)</span> (the slope and the intercept) and so the first <span class="math notranslate nohighlight">\(n-2\)</span> residuals are free to be any possible value, but the value of the final 2 are constrained by the estimated values of <span class="math notranslate nohighlight">\(\beta_{0}\)</span> and <span class="math notranslate nohighlight">\(\beta_{1}\)</span>. This also has a direct connection with Bessel’s correction, because estimating the variance of a sample requires an estimate of the mean. This constrains the number of independent pieces of information that can be used to estimate the variance from <span class="math notranslate nohighlight">\(n\)</span> to <span class="math notranslate nohighlight">\(n-1\)</span>.</p>
<p>The concept of degrees of freedom can be quite difficult to grasp. In fact, the origin of the concept came from Fisher’s geometric perspective on linear models. Unfortunately, this requires a much deeper understanding of linear models than we will have time to establish on this course. In brief, we conceptualise quantities such as the residuals as vectors (arrows) in multidimensional space. The number of dimensions these vectors occupy is given by the degrees of freedom. As such, to compare vectors that lie in different numbers of dimensions, we summarise them by producing an <em>average value per-dimension</em>. This makes the act of dividing quantities by their degrees of freedom make more sense, but this is not a perspective we would recommend trying to master at this stage. However, it is worth knowing that a more intuitive perspective on degrees of freedom does exist, if you want to look into it at a later date.</p>
</div>
<aside class="topic">
<p class="topic-title">What do you now know?</p>
<p>In this section, we have explored the method of Maximum Likelihood as a means of estimating the simple regression parameters using a sample of data. After reading this section, you should have a good sense of:</p>
<ul class="simple">
<li><p>Why MLE is a more general-purpose and useful approach to focus on, even though OLS is typically used for estimating basic linear models.</p></li>
<li><p>The concept that the likelihood function is really just a probability statement about the <em>data</em> given some estimates for the <em>parameters</em>.</p></li>
<li><p>The concept that evaluating the likelihood function involves taking the assumed distribution of the data, as well as some guess for the parameters, and then using that information to generate a probability for each data point. These probabilities are then multiplied together to provide an <em>overall</em> probability of the data we have collected.</p></li>
<li><p>The concept that <em>maximising</em> the likelihood involves finding guesses for the parameters that makes the probability of the data as <em>large</em> as possible.</p></li>
<li><p>The concept that there are <em>two</em> approaches to finding the parameters that maximise the likelihood. For well-behaved problems, exact solutions have already been found, whereas for more difficult problems, iterative algorithms need to be used to search for solutions.</p></li>
<li><p>The results that, for simple linear models assuming a normal distribution, the exact solutions for the slope and intercept are <em>identical</em> when using either MLE or OLS.</p></li>
<li><p>The result that MLE produces a <em>biased</em> estimate of the variance and thus needs to be corrected using <em>restricted maximum likelihood</em> (REML).</p></li>
</ul>
</aside>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="searlefoot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>This is the perspective taken by <a class="reference external" href="https://www.librarysearch.manchester.ac.uk/permalink/44MAN_INST/1r887gn/alma9930787964401631">McCulloch, Searle &amp; Neuhaus (2008)</a>, who are leading experts on the use of linear models within statistics.</p>
</aside>
<aside class="footnote brackets" id="bayesfoot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>This may seem like the wrong quantity. Surely, we are interested in <span class="math notranslate nohighlight">\(P(\boldsymbol{\theta}|\mathcal{D})\)</span>? In other words, finding the parameters that are most probable, given the data. Unfortunately, evaluating the probability <span class="math notranslate nohighlight">\(P(\boldsymbol{\theta}|\mathcal{D})\)</span> requires Bayesian methods and so cannot be evaluated from a purely Frequentist perspective.</p>
</aside>
<aside class="footnote brackets" id="likprobsfoot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>This is a problem of getting values of 0 due to issues with computational precision when working with many small probabilities. Taking logs not only changes the scale so that this does not happen, but it also turns <em>multiplication</em> into <em>summation</em>. Historically, this made calculating the likelihood much easier by hand. If you ever want to get back to the likelihood value, you can just undo the logs by using <code class="docutils literal notranslate"><span class="pre">exp(loglik)</span></code>.</p>
</aside>
<aside class="footnote brackets" id="minmaxfoot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>Making the <em>negative</em> log-likelihood <em>smaller</em> is the same as making the <em>positive</em> log-likelihood <em>bigger</em>. All we need to do to turn a maxmisation problem into a minimisation problem is to multiply the value of the objective function by <span class="math notranslate nohighlight">\(-1\)</span>.</p>
</aside>
</aside>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="2.simple-regression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The Simple Regression Model</p>
      </div>
    </a>
    <a class="right-next"
       href="4.estimated-model.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Estimated Model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-vs-ols">MLE vs OLS</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-mle-work">How Does MLE Work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood-function">The Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-concrete-example-in-r">A Concrete Example in <code class="docutils literal notranslate"><span class="pre">R</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-solutions">Exact Solutions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-mle">Iterative MLE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#restricted-maximum-likelihood">Restricted Maximum Likelihood</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr Martyn McFarquhar & Dr George Farmer
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2026.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>